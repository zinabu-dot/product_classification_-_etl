{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pyodbc\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from db_connection import *\n",
    "from googletrans import Translator\n",
    "from deep_translator import GoogleTranslator\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "#from sklearn import pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, precision_recall_curve, confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_category(x):\n",
    "    \"\"\" Split the category into different columns (max 9) \"\"\"\n",
    "    splitted = x.split('\"')\n",
    "    selected = list(map(splitted.__getitem__, list(range(1, min(18, len(splitted)), 2))))\n",
    "    while len(selected) < 9:\n",
    "        selected.append('N/A')\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiple_columns(df, column_name, prefix, drop_column=False):\n",
    "    \"\"\" Create multiple columns from the given column containing lists \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    lists = df_copy[column_name].to_list()\n",
    "    column_vals = list(zip(*lists))\n",
    "    for idx, c in enumerate(column_vals):\n",
    "        df_copy[f'{prefix}_{idx+1}'] = c\n",
    "    if drop_column:\n",
    "        df_copy.drop(column_name, axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_unicode(df, columns, values, replace_by):\n",
    "    \"\"\" Replace tagged unicode with the replacement values in the designated column(s) \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for v, r in zip(values, replace_by):\n",
    "        for c in columns:\n",
    "            df_copy[c] = df_copy[c].apply(lambda x: x.replace(v, r))\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_title(df, to_language='nl', brand_translate=False, drop_titles=True):\n",
    "    \"\"\" Translate the list of titles to the given language, keeping brands into account \"\"\"\n",
    "    # Ensure data copy and initialize looping variables and translation function\n",
    "    df_copy = df.copy()\n",
    "    translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "    time.sleep(2)\n",
    "    titles, brands = list(df_copy['title']), list(df_copy['brand'])\n",
    "    translated_titles = []\n",
    "\n",
    "    for idx, title in tqdm(enumerate(titles)):\n",
    "        \n",
    "        # If brand is given, look to remove it from title before translation\n",
    "        if str(brands[idx]) != '': #'nan'\n",
    "            title_splitted, brand_splitted = title.split(' '), brands[idx].split(' ')\n",
    "            cleaned_title, brand_idx = [], []\n",
    "            for idx, word in enumerate(title_splitted):\n",
    "                if word in brand_splitted:\n",
    "                    brand_idx.append(idx)\n",
    "                else:\n",
    "                    cleaned_title.append(word)\n",
    "            cleaned_title = ' '.join(cleaned_title)\n",
    "            \n",
    "            # Translated cleaned title and replace brand in title (order of words irrelevant)\n",
    "            if brand_translate:\n",
    "                translation = translator.translate(title, lang_tgt=to_language)\n",
    "                # print(translation)\n",
    "            else:\n",
    "                translation = translator.translate(cleaned_title, lang_tgt=to_language)\n",
    "                # print(translation)\n",
    "            translation_splitted = translation.split(' ')\n",
    "            translated_title = translation_splitted + brand_splitted\n",
    "            translated_title = ' '.join(translated_title)\n",
    "            \n",
    "        # If brand is not given simply translate and return translation\n",
    "        else:\n",
    "            translated_title = translator.translate(title, lang_tgt=to_language)\n",
    "    \n",
    "        translated_titles.append(translated_title)\n",
    "    \n",
    "    # Insert data in dataframe and drop title if requested\n",
    "    df_copy['title_nl'] = translated_titles \n",
    "    if drop_titles:\n",
    "        df_copy.drop('title', axis=1, inplace=True)\n",
    "    return df_copy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strings to connect to a database\n",
    "driver_server = 'host_server'   \n",
    "db_name = 'etl_db'\n",
    "user_name = 'etl_user_name' \n",
    "pasword = 'pa$$word'\n",
    "\n",
    "# connect to a database\n",
    "conn = connect_to_db(driver_server, db_name, user_name, pasword)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# select variables from a table in the given database\n",
    "tables = pd.read_sql(\"select * from 'table_name';\",conn)\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data processing starts here </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data frame\n",
    "df = tables.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disect category column and create separate columns for each (sub)category\n",
    "df['category'] = df['category'].apply(lambda x: split_category(x))\n",
    "df = create_multiple_columns(df=df, column_name='category', prefix='cat', drop_column=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform unicode to string values\n",
    "df = transform_unicode(df=df, \n",
    "                       columns=[col for col in df.columns if col[:3] == 'cat'], \n",
    "                       values=[\"\\\\u0026\", \"\\\\u0022\"], \n",
    "                       replace_by=[\"&\", \" inch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate the descriptions of products\n",
    "df = translate_title(df = df, \n",
    "                     to_language = 'nl', \n",
    "                     brand_translate = False, \n",
    "                     drop_titles = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter to lower case\n",
    "df[\"title_nl\"] = df[\"title_nl\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove product types with only few counts \n",
    "cats_list = [\"Kindle Store\", \"Software\", \"Alexa Skills\", \"Digital Music\", \n",
    "             \"Home & Business Services\", \"Handmade\", \n",
    "             \"Magazine Subscriptions\", \"Gift Cards\",\n",
    "             \"N/A\", \"Movies & TV\", \"CDs & Vinyl\"]#, \"Books\", \"Automotive\"]\n",
    "df = df.loc[~df.cat_1.isin(cats_list),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add manually labeled data from the local digital market to enrich labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve local data\n",
    "def localProductLineData(data_path):\n",
    "    \n",
    "    \"\"\"Get manually validated (labelled) product lines from mollie shops\"\"\"\n",
    "    \n",
    "    df_new = pd.read_excel(data_path)\n",
    "    df_new = df_new.loc[df_new.real_class != 'No Product',:]\n",
    "    df_new = df_new.dropna(subset=['Correct'])\n",
    "    display(df_new.Correct.value_counts())\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "data_dir = 'data_directory/data_name'\n",
    "df_new = localProductLineData(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels and pick only columns that match with the variables from the database data\n",
    "def get_Labels(data, x, y, z):\n",
    "    \n",
    "    for idx, row in data.iterrows():\n",
    "        if row[y] == 1.0:\n",
    "            data.loc[idx, 'cat_1'] = row[x]\n",
    "        else:\n",
    "            data.loc[idx, 'cat_1'] = row[z] \n",
    "    return data\n",
    "df_new = get_Labels(df_new, 'predicted_class', \n",
    "                       'Correct', 'real_class')\n",
    "df_new.rename(columns = {'Description': 'title_nl'}, \n",
    "                 inplace = True)\n",
    "df_prodLine = df_new[['title_nl', 'cat_1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get product lines datasets from a local directory that are related to (mono shops) shops that sell only a single product type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the path to excel files\n",
    "path = \"directory_for_files/*.xlsx\"\n",
    "\n",
    "def get_monoShops_prodline(path):\n",
    "    \n",
    "    # get files in the path\n",
    "    list_of_files = glob.glob(path)\n",
    "\n",
    "    # concatenate files\n",
    "    files_list = []\n",
    "\n",
    "    for file in list_of_files:\n",
    "        files_list.append(pd.read_excel(file))\n",
    "\n",
    "    # create an dataframe to store the concatenated data\n",
    "    Merchant_merged = pd.concat(files_list, ignore_index=True)\n",
    "\n",
    "    Merchant_merged.rename(columns={'Vertical':'cat_1',\n",
    "                                        'Description': 'title_nl'}, \n",
    "                              inplace = True)\n",
    "    \n",
    "    Merchant_merged[\"cat_1\"] = Merchant_merged[\"cat_1\"].str.\\\n",
    "                replace('Automotive/Transportation', 'Transportation')\n",
    "    \n",
    "    return Merchant_merged\n",
    "\n",
    "Merchant_prodline = get_monoShops_prodline(path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all datasets imported above\n",
    "df_amzMoll = pd.concat([df, df_prodLine, Merchant_prodline])\n",
    "df_amzMoll[\"title_nl\"] = df_amzMoll[\"title_nl\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode different names of the same label (class) for uniformity\n",
    "df_amzMoll[\"cat_1\"] = df_amzMoll[\"cat_1\"].str.replace('Automotive', 'Transportation')\n",
    "\n",
    "# remove classes that we dont need\n",
    "df_amzMoll = df_amzMoll[df_amzMoll[\"cat_1\"] != 'Erotics']\n",
    "\n",
    "\"\"\" Remove numbers \"\"\"\n",
    "df_amzMoll[\"title_nl\"] = df_amzMoll[\"title_nl\"].str.replace('\\d+','')\n",
    "df_amzMoll = df_amzMoll.dropna(subset = ['title_nl'])\n",
    "df_amzMoll.cat_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amzMoll[\"title_nl\"] = df_amzMoll[\"title_nl\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The data considered is completely text data, which is the description (title) of the products purchased by consumers. Our machine learning model could not directly use the text data, however, with the help of NLP, we can transform it into a scaleable (numeric) value.  \n",
    "To mention few of the actions to strike on, the removal of stopwords, lemmatization, Porter stemming, and the use of different algorithms, such as Term-Frequency Inverse Document Frequency (TF-IDF) are handy.\n",
    "On the other hand, there is a feed-forward kind approach called the basic CountVectorizer that leads to acceptable results. \n",
    "It takes into account the entire words in the titles of products and counts them, and then assigns a number to each one based on its prevalence in the dataset, \n",
    "creating a bag of words matrix required by the model. The final step is to convert it to a dense array so it can be used by the Naive Bayes object.\n",
    "\"\"\"\n",
    "\n",
    "count_vec = CountVectorizer(max_features=150000)\n",
    "bow = count_vec.fit_transform(df_amzMoll[\"title_nl\"]) \n",
    "# vocablaries = count_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get association overview between labels and features\n",
    "X_names = count_vec.get_feature_names_out()\n",
    "y = df_amzMoll[\"cat_1\"] \n",
    "p_value_limit = 0.95\n",
    "df_features = pd.DataFrame()\n",
    "for cat in np.unique(y):\n",
    "    chis2, p = feature_selection.chi2(bow, y==cat)\n",
    "    df_features = df_features.append(pd.DataFrame(\n",
    "                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "    df_features = df_features.sort_values([\"y\",\"score\"], \n",
    "                    ascending=[True,False])\n",
    "    df_features = df_features[df_features[\"score\"] > p_value_limit]\n",
    "X_names = df_features[\"feature\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in np.unique(y):\n",
    "    print(\"# {}:\".format(cat))\n",
    "    print(\"  . selected features:\",\n",
    "         len(df_features[df_features[\"y\"]==cat]))\n",
    "    print(\"  . top features:\", \",\".join(\n",
    "            df_features[df_features[\"y\"]==cat][\"feature\"].values[:10]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(vocabulary=X_names)\n",
    "# vectorizer.fit(df_amzMoll[\"title_nl\"])\n",
    "# X = vectorizer.transform(df_amzMoll[\"title_nl\"])\n",
    "# dic_vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (train and test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow, y, stratify = y, test_size=0.2)\n",
    "\n",
    "# classifier object\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# pipeline\n",
    "model = Pipeline([(\"vectorizer\", count_vec),  \n",
    "                           (\"classifier\", classifier)])\n",
    "# train classifier\n",
    "model[\"classifier\"].fit(X_train, y_train)\n",
    "\n",
    "# make prediction\n",
    "predicted = model[\"classifier\"].predict(X_test)\n",
    "predicted_prob = model[\"classifier\"].predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "auc = roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(classification_report(y_test, predicted))\n",
    "    \n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, predicted)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25, 25))\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "# Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              #label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "              #                auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               # label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "               #                    auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the classifier as python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prod_cat_ml V20-12-2022.pkl\", 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
